{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "get_ipython().system(u'pip install https://github.com/HIPS/autograd/archive/master.zip')",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting https://github.com/HIPS/autograd/archive/master.zip\n  Downloading https://github.com/HIPS/autograd/archive/master.zip\n\u001b[K     | 4.5MB 67.9MB/s\nRequirement already satisfied: numpy>=1.12 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from autograd==1.2) (1.14.6)\nRequirement already satisfied: future>=0.15.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from autograd==1.2) (0.16.0)\nBuilding wheels for collected packages: autograd\n  Building wheel for autograd (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-desfj_tv/wheels/1d/cf/71/e88cbaeb7af1fdd864ada20886075a6ffbc935d3b4eaf06a9a\nSuccessfully built autograd\nInstalling collected packages: autograd\nSuccessfully installed autograd-1.2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "get_ipython().system(u'pip install -U tensorly')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting tensorly\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/1a/d7805cdbafe391e0b3256a51ff2319d5ab2326fa1f7495ce53d574562a9a/tensorly-0.4.3-py3-none-any.whl (81kB)\n\u001b[K    100% |████████████████████████████████| 81kB 3.9MB/s ta 0:00:011\n\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorly) (1.1.0)\nRequirement already satisfied, skipping upgrade: nose in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorly) (1.3.7)\nRequirement already satisfied, skipping upgrade: numpy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorly) (1.14.6)\nInstalling collected packages: tensorly\nSuccessfully installed tensorly-0.4.3\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport autograd.numpy as anp\nfrom autograd import jacobian\nfrom autograd.scipy.misc import logsumexp\nfrom autograd.scipy.signal import convolve\nfrom scipy.linalg import norm\n\ndef softmax(x):    return anp.exp(x - logsumexp(x))\ndef softplus(x):   return anp.logaddexp(0., x)\ndef sigmoid(x):    return anp.reciprocal(anp.exp(softplus(-x)))\ndef flip(x):       return x[::-1]",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dim = 6",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def function_with_conv(x):\n    a = flip(x)\n    b = anp.sin(x)\n    c = a/b\n    d = np.convolve(c, np.array([1,-2,1]), mode='same') # autograd: Mode 'same' not yet implemented\n    e = 1./softmax(d)\n    return anp.inner(e,e)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def f1(x):\n    a = flip(x)\n    b = anp.sin(x)\n    return a / b\n\ndef conv(x):\n    return np.convolve(x, np.array([1, -2, 1]), mode='same')\n\n\ndef f2(x):\n    e = 1. / softmax(x)\n    return anp.inner(e, e)\n\n\ndef vjpForConvolve(v,x):\n    r=np.zeros(len(x))\n    r[0]=v[0]*(-2)+v[1]*1\n    r[len(x)-1]=v[len(v)-2]*1+v[len(v)-1]*(-2)\n\n    for i in range(len(x)-2):\n        r[i+1]=v[i]*1+v[i+1]*(-2)+v[i+2]*1\n    return r\n\ndef gradient(x):\n    f1x=f1(x)\n    f2j=jacobian(f2)(conv(f1x))\n    f1j=jacobian(f1)(x)\n    return (vjpForConvolve(f2j,f1x)).dot(f1j)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def approx_grad(f, x0):\n    epsilon = 1e-5\n    dim = len(x0)\n    grad = np.zeros((dim,)).T\n    ei   = np.zeros((dim,)).T\n    for k in range(dim):\n        ei[k] = epsilon\n        grad[k] = (f(x0 + ei) - f(x0 - ei))/(2.*epsilon)\n        ei[k] = 0.0\n    return grad\n\ndef check_gradient(f, gradf, x0):\n    num_grad = approx_grad(f, x0);\n    anal_grad = gradf(x0);\n    diff = norm(num_grad - anal_grad)/norm(num_grad + anal_grad)  #отношение норм разницы к сумме градиентов  \n    return diff\n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = np.random.randn(dim,)\ncheck_gradient(function_with_conv, gradient, x)\n",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "2.29142397809056e-06"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}